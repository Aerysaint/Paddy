{
    "title": "Attention Is All You Need",
    "outline": [
        {
            "level": "H1",
            "text": "Abstract",
            "page": 1
        },
        {
            "level": "H1",
            "text": "Introduction",
            "page": 1
        },
        {
            "level": "H1",
            "text": "Background",
            "page": 2
        },
        {
            "level": "H1",
            "text": "Model Architecture",
            "page": 3
        },
        {
            "level": "H2",
            "text": "Encoder and Decoder Stacks",
            "page": 3
        },
        {
            "level": "H2",
            "text": "Attention",
            "page": 3
        },
        {
            "level": "H3",
            "text": "Scaled Dot‑Product Attention",
            "page": 3
        },
        {
            "level": "H3",
            "text": "Multi‑Head Attention",
            "page": 4
        },
        {
            "level": "H3",
            "text": "Applications of Attention in our Model",
            "page": 4
        },
        {
            "level": "H2",
            "text": "Position‑wise Feed‑Forward Networks",
            "page": 5
        },
        {
            "level": "H2",
            "text": "Embeddings and Softmax",
            "page": 5
        },
        {
            "level": "H2",
            "text": "Positional Encoding",
            "page": 6
        },
        {
            "level": "H1",
            "text": "Why Self‑Attention",
            "page": 7
        },
        {
            "level": "H1",
            "text": "Training",
            "page": 8
        },
        {
            "level": "H2",
            "text": "Training Data and Batching",
            "page": 8
        },
        {
            "level": "H2",
            "text": "Hardware and Schedule",
            "page": 9
        },
        {
            "level": "H2",
            "text": "Optimizer",
            "page": 9
        },
        {
            "level": "H2",
            "text": "Regularization",
            "page": 9
        },
        {
            "level": "H1",
            "text": "Results",
            "page": 10
        },
        {
            "level": "H2",
            "text": "Machine Translation",
            "page": 10
        },
        {
            "level": "H2",
            "text": "Model Variations",
            "page": 11
        },
        {
            "level": "H2",
            "text": "English Constituency Parsing",
            "page": 12
        },
        {
            "level": "H1",
            "text": "Conclusion",
            "page": 13
        }
    ]
}