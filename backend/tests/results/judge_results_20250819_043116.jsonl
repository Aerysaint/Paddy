{"selection": "Generative adversarial networks involve two competing neural networks: a generator that produces samples and a discriminator that estimates the probability that a sample came from the training data rather than the generator. The training process is framed as a minimax game between these two models.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/e30f3aab2c794949a54d592058b6f294_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/e30f3aab2c794949a54d592058b6f294_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The `home` document is an excellent match for the selection. It accurately describes Generative Adversarial Networks, the roles of the generator and discriminator, and the minimax game framework, directly supporting all points in the selection. The `doc_match` is also perfect, as the file name matches the expected content. The `coverage` is complete, as all aspects of the selection are covered by the `home` document. However, the `related` documents are identical copies of the `home` document, providing no additional or helpful context. This redundancy significantly diminishes the value of the 'related' section, leading to a low `related_support` score. Despite the flaw in related documents, the core information retrieval (home document) is outstanding."}}
{"selection": "The generator aims to capture the data distribution, while the discriminator learns to distinguish between real and fake samples. This adversarial process helps the generator produce increasingly realistic outputs.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/e30f3aab2c794949a54d592058b6f294_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/e30f3aab2c794949a54d592058b6f294_gan_1406.2661.pdf", "doc_match": 5, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The `doc_match` is excellent because the system correctly identified the original GAN paper (1406.2661.pdf). However, the `relevance` of the provided `home.text` to the `selection` is extremely poor. The selection perfectly describes the core concept of GANs (generator vs. discriminator, adversarial process), but the home text snippet discusses alternative generative models like NCE, with no mention of the specific adversarial interaction or roles of generator and discriminator as described. Since the `home.text` is irrelevant and no `related` documents were provided, `coverage` and `related_support` are also minimal. The overall score is low because while the correct paper was identified, the system failed to retrieve a relevant passage from it to support the given selection."}}
{"selection": "We introduce deep residual learning frameworks that explicitly reformulate the layers as learning residual functions with reference to the layer inputs, rather than learning unreferenced functions. These residual connections allow for training of extremely deep neural networks.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/264ae972032d49108cf18f087a273e2e_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/264ae972032d49108cf18f087a273e2e_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The `home` document correctly matches the expected file and its `text` (abstract) is highly relevant, directly addressing the core concepts in the `selection` regarding residual learning and its application to deep neural networks. The `home` document alone provides full `coverage` of the `selection`'s content. However, all `related` documents are identical copies of the `home` document's abstract, offering no additional or distinct helpful context, thus scoring low on `related_support`. The overall score reflects the excellent primary retrieval but notes the redundancy in the related documents."}}
{"selection": "The core idea is to add a 'shortcut connection' that bypasses one or more layers, performing an identity mapping and adding its output to the output of the stacked layers. This addresses the degradation problem in very deep networks.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/264ae972032d49108cf18f087a273e2e_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/264ae972032d49108cf18f087a273e2e_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document is an excellent match for the selection, perfectly explaining the concept of shortcut connections, identity mapping, and how they address the degradation problem in deep networks. The document filename also correctly matches the expected file. However, all the 'related' documents provided are exact duplicates of the home document's text snippet, offering no additional or different context. This redundancy significantly diminishes the value of the 'related' section, despite the strong performance of the primary retrieval."}}
{"selection": "BERT, or Bidirectional Encoder Representations from Transformers, is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. It leverages a Masked Language Model and Next Sentence Prediction task.", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document correctly identifies the BERT paper (1810.04805.pdf). The extracted text from the home document is highly relevant and directly confirms all aspects mentioned in the selection, including the use of Masked Language Model, Next Sentence Prediction, and the pre-training of deep bidirectional representations by conditioning on both left and right context. The coverage of the selection's information by the home text is complete. No related documents were provided."}}
{"selection": "Unlike previous models, BERT allows the model to learn context from both sides of a token simultaneously. This pre-trained model can then be fine-tuned with just one additional output layer for a wide range of NLP tasks.", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 3, "overall": 4, "feedback": "The home document is the correct paper (BERT) and its abstract directly and completely supports the selection. It explicitly states BERT's bidirectional nature and its ability to be fine-tuned with one additional output layer for a wide range of tasks. The related documents are exact duplicates of the home document's abstract, providing no additional context or support beyond what the home document already offers. While not unhelpful, they are redundant."}}
{"selection": "We propose a novel network architecture, the Transformer, based solely on attention mechanisms, eschewing recurrence and convolutions entirely. This architecture yields significantly better performance on machine translation tasks.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 2, "overall": 4, "feedback": "The home document perfectly matches the expected file and the selection text is almost verbatim from the home document, indicating excellent relevance and coverage from the primary result. However, the 'related' document is an exact duplicate of the 'home' document, providing no additional context or support, which reduces the related_support score."}}
{"selection": "The Transformer relies on a self-attention mechanism, which allows it to draw global dependencies between input and output. This parallelism enables faster training compared to recurrent or convolutional networks.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document perfectly matches the expected file and provides excellent support for the selection. It explicitly states that the Transformer relies on an attention mechanism to draw global dependencies, allows for more parallelization, and eschews recurrence, leading to faster training. The coverage of the selection's ideas by the home document alone is comprehensive. However, the related document is a truncated duplicate of the home document and offers no additional or distinct helpful context, which significantly lowers its support score. While the primary information is very strong, the redundancy in related documents is a system weakness."}}
{"selection": "We show that a pure Transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data, Vision Transformers achieve excellent results.", "expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/f52dce91115a47f38d3169989511afa7_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes Vision Transformers (ViT) applied to sequences of image patches for image classification. The retrieved home document and all related documents are about BERT, which is a Transformer-based language model. While BERT uses Transformers and pre-training, it is for natural language processing, not computer vision tasks involving image patches or image classification. The documents retrieved are entirely from the wrong domain, making them irrelevant to the selection's specific content."}}
{"selection": "The approach involves splitting an image into fixed-size patches, linearly embedding each patch, and adding position embeddings before feeding the resulting sequence of vectors to a standard Transformer encoder.", "expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes the core methodology of a Vision Transformer (ViT), specifically how images are processed (splitting into fixed-size patches, linear embedding, adding position embeddings, and feeding to a Transformer encoder). The home and all related documents are from the original 'Attention Is All You Need' paper (1706.03762), which introduces the Transformer architecture but does not discuss image processing in this manner or the ViT approach. The retrieved documents are completely irrelevant to the specific details in the selection. The expected document (1910.07470v1.pdf) was present in the list of available documents but not retrieved as home or related."}}
{"selection": "We introduce DALL-E, a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text-image pairs. The model is able to create diverse and novel images.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/6f30cf7ff70b47de915036c631ebe9ff_2010.13852v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/6f30cf7ff70b47de915036c631ebe9ff_2010.13852v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes DALL-E, a model for generating images from text. The retrieved home document and all related documents are about BERT, a language model focused on text pre-training and fine-tuning. There is no conceptual overlap, and the retrieved documents are completely irrelevant to the selection. The system failed to provide any useful or correct information."}}
{"selection": "DALL-E leverages a discrete variational autoencoder to compress images into a sequence of tokens, which are then modeled by an autoregressive Transformer. This allows for joint modeling of text and image tokens.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a3479ad214fc477493d86a3734a12169_attention_is_all_you_need_1706.03762.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes the architecture of DALL-E, specifically mentioning a discrete variational autoencoder and joint modeling of text and image tokens. The home document is 'Attention Is All You Need' (the Transformer paper), which is a foundational paper but does not describe DALL-E, VAEs, or joint text/image modeling. While DALL-E uses a Transformer, the retrieved document is not specific to DALL-E's architecture as described in the selection. All related documents are identical to the home document, providing no additional relevant context. The expected document (DALL-E paper, 2010.13852) was not retrieved."}}
{"selection": "We introduce a neural algorithm of artistic style that separates and recombines content and style from arbitrary images. This is achieved by optimizing a new image to match the content representation of one image and the style representation of another.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The retrieved home document (1910.07470v1.pdf) is completely incorrect and irrelevant to the selection. The selection describes a 'neural algorithm of artistic style' for image manipulation, while the retrieved document discusses 'radiomics' and 'deep learning' applied to 'brain diseases' and 'glioblastoma multiforme'. The expected document (1508.02986) was not retrieved. The related documents are also from the same irrelevant file and continue the theme of medical imaging and brain diseases, providing no support for the selection."}}
{"selection": "The method uses pre-trained convolutional neural networks to extract image features at different layers, defining content and style representations. Gradient descent is then used to synthesize an image that minimizes a loss function combining these representations.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The retrieved 'home' document (1910.07470v1.pdf) is completely incorrect. The selection describes the neural style transfer method (using pre-trained CNNs, content/style representations, and gradient descent for image synthesis), which is characteristic of the paper by Gatys et al. (1508.02986). The retrieved document, however, discusses deep learning in radiomics for brain disease prediction and tumor segmentation, which is entirely unrelated to the selection's content. The 'related' documents are from the same irrelevant paper and therefore provide no helpful context or coverage."}}
