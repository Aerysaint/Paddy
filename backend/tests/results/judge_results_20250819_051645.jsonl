{"selection": "We propose a new framework for estimating generative models via an adversarial process, where a generator network and a discriminator network are simultaneously trained.", "expected_file_contains": "1406.2661", "expected_file_contains_any": ["1406.2661"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match_auto": true, "doc_match_hit": "1406.2661", "judge": {"expected_file_contains": "1406.2661", "expected_file_contains_any": ["1406.2661"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 5, "overall": 5, "feedback": "The home document is the exact paper identified by the selection, and the provided text is the abstract which perfectly matches and elaborates on the selection. The related document provides excellent contextual support by explaining the motivation for this new framework in deep learning, adding valuable, non-redundant information. All aspects of the retrieval are outstanding."}}
{"selection": "The generative adversarial network consists of two models: a generator G that captures the data distribution, and a discriminator D that estimates the probability that a sample came from the training data rather than G.", "expected_file_contains": "1406.2661", "expected_file_contains_any": ["1406.2661"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match_auto": true, "doc_match_hit": "1406.2661", "judge": {"expected_file_contains": "1406.2661", "expected_file_contains_any": ["1406.2661"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 5, "overall": 5, "feedback": "The home document perfectly matches the selection, providing the exact definition of a GAN. The document is correctly identified as 'gan_1406.2661.pdf'. The related document, also from the same paper, offers excellent complementary context by explaining the motivation behind proposing this new framework, enhancing the overall understanding without redundancy."}}
{"selection": "We reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions, to address the degradation problem in very deep networks.", "expected_file_contains": "1512.03385", "expected_file_contains_any": ["1512.03385"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match_auto": true, "doc_match_hit": "1512.03385", "judge": {"expected_file_contains": "1512.03385", "expected_file_contains_any": ["1512.03385"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 5, "overall": 5, "feedback": "The home document is the correct paper (ResNet, 1512.03385). The text in the home document's abstract is a near-perfect, direct match to the selection, explicitly stating the reformulation of layers as learning residual functions. The related text from the same document provides further excellent context and explanation of the residual learning concept, supporting the selection fully. The solution for the 'degradation problem' is strongly implied by 'easier to optimize, and can gain accuracy from considerably increased depth' in the abstract and the overall purpose of ResNets for very deep networks."}}
{"selection": "Deep residual networks explicitly allow for identity mapping shortcuts, which help in training very deep neural networks by alleviating the degradation problem and vanishing gradients.", "expected_file_contains": "1512.03385", "expected_file_contains_any": ["1512.03385"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match_auto": true, "doc_match_hit": "1512.03385", "judge": {"expected_file_contains": "1512.03385", "expected_file_contains_any": ["1512.03385"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 4, "coverage": 3, "related_support": 1, "overall": 3, "feedback": "The home document correctly identifies the paper on ResNets and the provided text is highly relevant, explaining how identity mapping shortcuts in deep residual networks address the degradation problem and aid in training very deep networks. However, the provided text snippet does not explicitly mention 'vanishing gradients', which is a specific term included in the selection as a problem alleviated by ResNets. Since no related documents were provided, the coverage for this specific term is missing, and related support is minimal."}}
{"selection": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is designed to pre-train deep bidirectional representations from unlabeled text.", "expected_file_contains": "1810.04805", "expected_file_contains_any": ["1810.04805"], "predicted_file_url": "/files/library/11104986442e4f34b11bd3a099c8390f_bert_1810.04805.pdf", "doc_match_auto": true, "doc_match_hit": "1810.04805", "judge": {"expected_file_contains": "1810.04805", "expected_file_contains_any": ["1810.04805"], "predicted_file_url": "/files/library/11104986442e4f34b11bd3a099c8390f_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 4, "overall": 5, "feedback": "The home document is the exact paper for BERT, and its text directly and completely supports the selection, providing the full name and the core design principle. The related text, also from the same paper, further elaborates on *how* BERT achieves its bidirectional representations (Masked Language Model) and contrasts it with other models, providing excellent additional context to the selection's statement. The document match is perfect."}}
{"selection": "We introduce two novel unsupervised prediction tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP), used for pre-training a deep bidirectional Transformer.", "expected_file_contains": "1810.04805", "expected_file_contains_any": ["1810.04805"], "predicted_file_url": "/files/library/11104986442e4f34b11bd3a099c8390f_bert_1810.04805.pdf", "doc_match_auto": true, "doc_match_hit": "1810.04805", "judge": {"expected_file_contains": "1810.04805", "expected_file_contains_any": ["1810.04805"], "predicted_file_url": "/files/library/11104986442e4f34b11bd3a099c8390f_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The `home` document (`bert_1810.04805.pdf`) perfectly matches the `expected_file_contains`. The text provided from the `home` document directly and clearly states the introduction of 'masked language model' (MLM) and 'next sentence prediction' (NSP) tasks, and their use for 'pre-training a deep bidirectional Transformer', fully aligning with the `selection`. Therefore, relevance and coverage are excellent. No `related` documents were provided, resulting in a low `related_support` score. The `overall` score reflects the strong performance of the `home` document retrieval combined with the absence of supporting related documents."}}
{"selection": "We propose the Transformer, a novel network architecture eschewing recurrence and convolutions entirely, relying solely on an attention mechanism to draw global dependencies between input and output.", "expected_file_contains": "1706.03762", "expected_file_contains_any": ["1706.03762"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "doc_match_hit": "1706.03762", "judge": {"expected_file_contains": "1706.03762", "expected_file_contains_any": ["1706.03762"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 5, "feedback": "The `home` document perfectly matches the expected file and the `home.text` directly contains the selected passage with nearly identical phrasing. The text from the `home` document fully covers the idea presented in the selection. There were no `related` documents provided, hence the low related_support score, but this does not detract from the excellent primary retrieval."}}
{"selection": "The Transformer uses multi-head self-attention, allowing the model to jointly attend to information from different representation subspaces at different positions.", "expected_file_contains": "1706.03762", "expected_file_contains_any": ["1706.03762"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "doc_match_hit": "1706.03762", "judge": {"expected_file_contains": "1706.03762", "expected_file_contains_any": ["1706.03762"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 5, "overall": 5, "feedback": "The home document is correct and the home text directly contains the core statement of the selection. The related text perfectly complements the home text by elaborating on how the Transformer uses multi-head attention, specifically mentioning self-attention in the encoder, which directly addresses the 'self-attention' part of the selection. Excellent retrieval."}}
{"selection": "We present a class of generative models called denoising diffusion probabilistic models, which are inspired by non-equilibrium thermodynamics.", "expected_file_contains": "1910.07470v1", "expected_file_contains_any": ["1910.07470v1"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "1910.07470v1", "expected_file_contains_any": ["1910.07470v1"], "predicted_file_url": "/files/library/c847a4ee3fde4943bc927c14c302c352_gan_1406.2661.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection is about 'denoising diffusion probabilistic models' and 'non-equilibrium thermodynamics'. The retrieved home document (gan_1406.2661.pdf) is about 'Adversarial nets' (GANs), which is a different type of generative model. The `expected_file_contains` was '1910.07470v1', which is the paper 'Denoising Diffusion Probabilistic Models'. The retrieved document is completely incorrect based on the specific topic of the selection. Therefore, doc_match, relevance, coverage, and related_support are all 1, as the retrieved content is entirely irrelevant to the selection."}}
{"selection": "These models learn to reverse a fixed Markov chain of diffusion steps, gradually denoising data until it becomes pure noise, and then reversing the process to generate new data.", "expected_file_contains": "1910.07470v1", "expected_file_contains_any": ["1910.07470v1"], "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "1910.07470v1", "expected_file_contains_any": ["1910.07470v1"], "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection accurately describes Diffusion Models, specifically their process of reversing a fixed Markov chain of diffusion steps to generate data. However, the `home` document provided (`gan_1406.2661.pdf`) is about Generative Adversarial Networks (GANs) and other generative model training techniques like score matching and NCE. While 'denoising auto-encoders' are mentioned, the context does not align with the specific 'diffusion steps' mechanism described in the selection. The `expected_file_contains` field correctly identifies `1910.07470v1.pdf` (Denoising Diffusion Probabilistic Models) as the relevant paper, which was available in the provided document list. Therefore, the selected home document is incorrect and entirely irrelevant to the core concept of the selection."}}
{"selection": "We demonstrate that a pure Transformer applied directly to sequences of image patches can perform very well on image classification tasks.", "expected_file_contains": "2010.13852v1", "expected_file_contains_any": ["2010.13852v1"], "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "2010.13852v1", "expected_file_contains_any": ["2010.13852v1"], "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The retrieved home document (1910.07470v1.pdf) is entirely incorrect for the provided selection. The selection describes the core concept of the Vision Transformer (ViT) paper (2010.13852v1.pdf), which applies a pure Transformer to sequences of image patches for image classification. The retrieved document, however, discusses radiomics and deep learning methods (like CNNs and CRFs) for medical image analysis, specifically for brain diseases and survival prediction, not general image classification with Transformers. Although 'image patches' are mentioned, the context and technology are completely different. Neither the home nor the related text provides any relevant information about the selection."}}
{"selection": "Our Vision Transformer (ViT) model processes an image by splitting it into fixed-size patches, linearly embedding each patch, and feeding the resulting sequence of vectors to a standard Transformer encoder.", "expected_file_contains": "2010.13852v1", "expected_file_contains_any": ["2010.13852v1"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "2010.13852v1", "expected_file_contains_any": ["2010.13852v1"], "predicted_file_url": "/files/library/4d90967b390f42e0aacc8845ed8255e7_attention_is_all_you_need_1706.03762.pdf", "doc_match": 1, "relevance": 2, "coverage": 1, "related_support": 2, "overall": 1, "feedback": "The selection describes a Vision Transformer (ViT) model, which is typically associated with the paper 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale' (likely 2010.13852v1). The `home` document provided is 'Attention Is All You Need' (1706.03762.pdf), which introduces the general Transformer architecture but not ViT itself. Therefore, the `doc_match` is incorrect.\n\nWhile ViT *uses* a Transformer encoder, the `home` text does not mention images, patches, or linear embedding, which are crucial specifics of the ViT model described in the selection, leading to low `relevance` and `coverage`. The `related` document (BERT) further reinforces the general Transformer, but adds no context about ViT's image processing, thus providing limited `related_support` for the selection as a whole."}}
{"selection": "We introduce Batch Normalization, a new method that reduces internal covariate shift, thereby accelerating the training of deep neural networks.", "expected_file_contains": "1508.02986v1", "expected_file_contains_any": ["1508.02986v1"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "1508.02986v1", "expected_file_contains_any": ["1508.02986v1"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match": 2, "relevance": 4, "coverage": 1, "related_support": 1, "overall": 2, "feedback": "The selection is a direct quote from the abstract/introduction of the Batch Normalization paper. The system retrieved the `resnet_1512.03385.pdf` document as 'home', which *cites* the Batch Normalization paper. While the citation entry itself contains the exact phrasing from the selection, the retrieved document is not the original source paper (1508.02986v1.pdf) which was expected. Therefore, the 'doc_match' is poor as the system found a citing paper instead of the source. The 'relevance' of the home text is high because the citation text perfectly matches the selection's core idea. However, 'coverage' is poor because neither the home document nor the related document (which is also the ResNet paper) actually explains Batch Normalization; they only cite it or discuss unrelated topics. 'Related_support' is also poor as the related document provides no relevant context to Batch Normalization."}}
{"selection": "Batch Normalization normalizes the inputs to each layer for each mini-batch, allowing for the use of much higher learning rates and making initialization less critical.", "expected_file_contains": "1508.02986v1", "expected_file_contains_any": ["1508.02986v1"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match_auto": false, "doc_match_hit": null, "judge": {"expected_file_contains": "1508.02986v1", "expected_file_contains_any": ["1508.02986v1"], "predicted_file_url": "/files/library/466bc3433c8649cbb7f97093b68c23e4_resnet_1512.03385.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection provides a definition and benefits of Batch Normalization. The retrieved 'home' and 'related' documents (both from the ResNet paper) only state that Batch Normalization is *used* and cite the original BN paper (which is the 'expected_file_contains'). They do not contain any of the descriptive information or benefits mentioned in the selection. Therefore, the home document is not a match for the content of the selection, and the related document provides no additional useful context."}}
