{"selection": "We propose a new framework for estimating generative models via an adversarial process, where two models, a generative model G and a discriminative model D, are simultaneously trained. The generator learns to capture the data distribution, while the discriminator estimates the probability a sample came from the training data versus the generator.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/275ae0b6c6474f49ad627afa4dca13ef_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/275ae0b6c6474f49ad627afa4dca13ef_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 2, "overall": 4, "feedback": "The `home` document is the correct paper and its abstract perfectly matches the provided `selection`, achieving excellent relevance and full coverage of the selection's content. However, the `related` documents offer limited additional value. One `related` document is an exact duplicate of the `home` document's text, providing no new information. The other `related` document provides general introductory context about the paper but does not specifically elaborate on the mechanics of G and D as described in the `selection`."}}
{"selection": "The generative adversarial network involves a minimax two-player game, where the generator tries to produce samples indistinguishable from real data, and the discriminator aims to correctly classify real versus generated samples.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/275ae0b6c6474f49ad627afa4dca13ef_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/275ae0b6c6474f49ad627afa4dca13ef_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document is the correct foundational paper for Generative Adversarial Networks (GANs) and its text perfectly explains the roles of the generator and discriminator, and their adversarial training, which embodies the minimax two-player game described in the selection. The coverage of the idea is excellent. However, the 'related' document is an exact duplicate of the 'home' document, providing no additional context or support."}}
{"selection": "We reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. This approach addresses the degradation problem observed in very deep neural networks.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 4, "overall": 5, "feedback": "The home document is an excellent match, containing the exact phrasing from the selection regarding the reformulation of layers as residual functions and clearly stating that this addresses the difficulty of training deeper networks. The related documents, while one is a duplicate of the home abstract, include a crucial section (3.1) that provides a more formal definition of the residual function (F(x) = H(x) - x), adding valuable technical context to the 'reformulation' aspect of the selection. This retrieval provides comprehensive and highly relevant information."}}
{"selection": "Deep residual networks utilize shortcut connections that perform identity mapping, adding their outputs to the outputs of stacked layers. This allows for training extremely deep neural networks more effectively by easing optimization.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 4, "overall": 5, "feedback": "The `home` document correctly identifies 'resnet_1512.03385.pdf' and its text perfectly matches the selection, explaining shortcut connections, identity mapping, output addition, and ease of optimization for deep residual networks. The `related` documents include one that is largely redundant with the `home` text, but another `related` document provides excellent additional context by explaining the formal `F(x)+x` formulation and explicitly stating the hypothesis that it's easier to optimize the residual mapping, which reinforces the 'easing optimization' aspect of the selection. Overall, the retrieval is highly accurate and comprehensive."}}
{"selection": "BERT, which stands for Bidirectional Encoder Representations from Transformers, is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 4, "overall": 5, "feedback": "The home document is the exact source of the selection, providing a perfect match. The related documents reinforce the information and provide excellent additional context, particularly regarding the importance of bidirectional pre-training and how BERT achieves it, contrasting it with other models. One related document is a duplicate of the home document's abstract, which is less helpful than unique context, but the other related document adds significant value."}}
{"selection": "We introduce two novel unsupervised prediction tasks for pre-training: Masked Language Model (MLM) and Next Sentence Prediction (NSP). These tasks enable BERT to learn rich representations for a wide range of downstream tasks.", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 4, "overall": 4, "feedback": "The home document correctly identifies the paper and directly discusses the Masked Language Model (MLM) and Next Sentence Prediction (NSP) as pre-training tasks for BERT. The second related document provides excellent supplementary context, explaining the importance of these tasks by detailing how their removal impacts downstream performance. The first related document is a duplicate of the home document, which is redundant but doesn't detract from the overall quality of the information provided."}}
{"selection": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. This model achieves state-of-the-art results on machine translation tasks.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/3b54801aa4c44037ab6fd1443d482297_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/3b54801aa4c44037ab6fd1443d482297_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document is perfectly matched to the expected file and provides excellent, verbatim relevance and full coverage of the selection. However, the 'related' document is an exact duplicate of the home document, offering no additional support or context. This redundancy significantly detracts from the utility of the related documents."}}
{"selection": "The Transformer relies heavily on multi-head self-attention, which allows the model to jointly attend to information from different representation subspaces at different positions. This mechanism is crucial for capturing long-range dependencies efficiently.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/3b54801aa4c44037ab6fd1443d482297_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/3b54801aa4c44037ab6fd1443d482297_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 4, "coverage": 5, "related_support": 4, "overall": 5, "feedback": "The home document correctly identifies the paper 'Attention Is All You Need'. The home text directly matches the part of the selection about multi-head attention allowing the model to jointly attend to information from different representation subspaces at different positions. The related documents significantly enhance the coverage: Related 2 confirms the Transformer's heavy reliance on multi-head attention, and Related 3 explicitly links multi-head attention to counteracting issues with learning 'dependencies between distant positions,' thus supporting the 'capturing long-range dependencies efficiently' aspect. While Related 1 is a duplicate of the relevant home text, the other related documents provide excellent, necessary context and support, leading to full coverage of the selection."}}
{"selection": "We present a unified text-to-text transformer model, T5, that treats every NLP problem as a text-to-text task. This framework allows for training on a diverse range of tasks using the same model, loss function, and hyperparameters.", "expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes T5, a 'unified text-to-text transformer model'. The home document and all related documents are from the BERT paper (bert_1810.04805.pdf). The retrieved content discusses BERT's pre-training and fine-tuning, which is completely irrelevant to the T5 model and its specific 'text-to-text' framework described in the selection. The expected file (1910.07470) is the T5 paper, which was not retrieved."}}
{"selection": "Our empirical study explores the limits of transfer learning by pre-training on a massive C4 dataset and fine-tuning on various downstream tasks. We achieve state-of-the-art results across many benchmarks.", "expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/30ec47faa4404cc9b476d34d8d6f389c_bert_1810.04805.pdf", "doc_match": 1, "relevance": 2, "coverage": 2, "related_support": 3, "overall": 1, "feedback": "The selection explicitly mentions a 'massive C4 dataset'. This dataset was introduced with the T5 paper (arXiv:1910.07470), not the BERT paper (arXiv:1810.04805) which was retrieved. While the BERT paper discusses transfer learning, pre-training, fine-tuning, and state-of-the-art results, it does not use the C4 dataset. Therefore, the retrieved document is fundamentally incorrect for the specific details provided in the selection, leading to a low score for doc_match, relevance, and coverage. All 'related' documents are also from the same incorrect paper, providing no corrective context."}}
{"selection": "We introduce DALL·E, a 12-billion parameter version of GPT-3 trained to generate images from text descriptions. It can create photorealistic images as well as images that do not exist in the real world.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/2ba88d7251d34085b0984c35b926b904_resnet_1512.03385.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection is about DALL·E, a text-to-image generation model. The retrieved home document and all related documents are about ResNet, Faster R-CNN, and object detection on PASCAL VOC and MS COCO datasets. These topics are completely unrelated to the selection. The system failed to retrieve any relevant information. The `expected_file_contains` '2010.13852' suggests a different paper should have been retrieved, and the current `predicted_file_url` does not contain this string."}}
{"selection": "DALL·E leverages a discrete variational autoencoder for image tokenization and a Transformer for joint modeling of text and image tokens. This architecture enables diverse and high-quality image generation from arbitrary text prompts.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/b337806aa8bb4d0ea151b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/b337806aa8bb4d0ea151b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes DALL·E's architecture, which is known to be from the 2010.13852 paper. However, the `home` document provided is 1910.07470v1.pdf, which is explicitly about radiomics, deep learning for medical imaging, and brain diseases. This document is entirely irrelevant to DALL·E. Both the `home` text and the `related` texts discuss medical image analysis and have no connection to the DALL·E architecture or text-to-image generation. The `doc_match` is incorrect, and the content relevance and coverage are nil."}}
{"selection": "We propose Batch Normalization, a technique to reduce internal covariate shift and accelerate the training of deep neural networks. It normalizes layer inputs by re-centering and re-scaling them.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/6f30cf7ff70b47de915036c631ebe9ff_2010.13852v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/6f30cf7ff70b47de915036c631ebe9ff_2010.13852v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The `selection` describes 'Batch Normalization', a specific technique for deep neural networks. The `home` document and all `related` documents are from '2010.13852v1.pdf', which is titled 'Seven deep learning algorithms for intrusion detection'. The content of these retrieved texts discusses various deep learning models (RNN, DNN, CNN, RBM, DBN, DBM, DA) in the context of intrusion detection, dataset performance, and concepts like imbalanced datasets and SMOTE. While one related snippet mentions 'feature normalization', this is a general concept and not the specific 'Batch Normalization' described in the selection (which is defined by 're-centering and re-scaling layer inputs'). The expected document, '1508.02986', is the seminal paper on Batch Normalization. The retrieved document is completely irrelevant to the selection's topic."}}
{"selection": "Batch Normalization allows for the use of much higher learning rates and makes initialization less critical. It also acts as a regularizer, sometimes eliminating the need for Dropout.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/9b408d6989d74337bc5df95b0cc8d5e7_1508.02986v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/9b408d6989d74337bc5df95b0cc8d5e7_1508.02986v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The `home` document provided (ResNet, 1512.03385) is not the expected document (Batch Normalization, 1508.02986). The selection describes the *benefits* of Batch Normalization, which are typically detailed in the original BN paper. The provided `home` and `related` texts from the ResNet paper only state that Batch Normalization is used and that dropout is not, but they do not explain *why* BN allows for higher learning rates, makes initialization less critical, or acts as a regularizer, as claimed in the selection. Therefore, the content does not match the specific claims of the selection, and the related documents do not add helpful context regarding these claims."}}
