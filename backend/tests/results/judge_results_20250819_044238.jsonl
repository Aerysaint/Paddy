{"selection": "We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/59fbc73aa15d43a591a93e1d8c44ba3f_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/59fbc73aa15d43a591a93e1d8c44ba3f_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document accurately matches the expected file and the selection is an exact excerpt from the home document's abstract, leading to perfect scores for doc_match, relevance, and coverage. However, all related documents are identical copies of the home document (same title, same abstract text), providing no additional helpful context or information, hence the low score for related_support."}}
{"selection": "The generative model G is trained to maximize the probability of D making a mistake, while D is trained to distinguish between real and fake samples, playing a minimax game.", "expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/59fbc73aa15d43a591a93e1d8c44ba3f_gan_1406.2661.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1406.2661", "predicted_file_url": "/files/library/59fbc73aa15d43a591a93e1d8c44ba3f_gan_1406.2661.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 3, "overall": 5, "feedback": "The home document is the correct paper (Generative Adversarial Nets) and its abstract perfectly matches and explains the entire selection. All aspects of the selection regarding G, D, distinguishing samples, maximizing D's mistake, and the minimax game are directly stated in the home document. The related documents are identical copies of the home document, so they do not provide additional context or coverage beyond what the home document already delivers perfectly."}}
{"selection": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/dad282b7301d4cb19af030e0a4340ab2_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/dad282b7301d4cb19af030e0a4340ab2_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document perfectly matches the selection and the expected file. The relevance is excellent as the selection is the opening sentence of the home document's abstract. The idea is fully covered by the home document. However, the 'related' documents are all identical copies of the home document, providing no additional or helpful context whatsoever. This redundancy significantly detracts from the value of the 'related' section."}}
{"selection": "Instead of hoping that each few stacked layers directly fit a desired underlying mapping, we explicitly reformulate the layers as learning a residual function with reference to the layer inputs.", "expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/dad282b7301d4cb19af030e0a4340ab2_resnet_1512.03385.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1512.03385", "predicted_file_url": "/files/library/dad282b7301d4cb19af030e0a4340ab2_resnet_1512.03385.pdf", "doc_match": 5, "relevance": 5, "coverage": 4, "related_support": 1, "overall": 3, "feedback": "The `home` document is definitively the source of the `selection`, with the provided text being an almost verbatim match. The `home` text also provides excellent context for the selected idea, explaining the 'why' and 'how' of residual learning. However, no `related` documents were provided, which significantly impacts the `related_support` score and limits the overall `coverage` to what the single `home` document can offer."}}
{"selection": "We introduce BERT, a new language representation model designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document correctly identifies the file (BERT paper) and its text perfectly matches and supports the given selection. The information in the home document alone provides complete coverage of the selection. However, all listed 'related' documents are exact duplicates of the home document, offering no additional or unique context, which significantly diminishes their utility as 'related support'."}}
{"selection": "BERT is pre-trained using two novel unsupervised prediction tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP).", "expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1810.04805", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document is the correct paper (BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding) and directly confirms that BERT is pre-trained using 'masked LM' (MLM) and 'next sentence prediction' (NSP) tasks. This perfectly matches the selection. The related documents are exact duplicates of the home document's text snippet, providing no additional context or support, which is why related_support is low. However, since the home document fully and accurately answers the selection, the overall retrieval of information is very strong."}}
{"selection": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/2061ea5b43b8478f98abe636d8947484_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/2061ea5b43b8478f98abe636d8947484_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The home document perfectly matches the expected file and contains the exact selection, making it highly relevant and fully covering the information. However, the 'related' document is an exact duplicate of the home document's content, providing no additional or distinct supporting context, which significantly lowers the related_support score due to redundancy."}}
{"selection": "The Transformer model employs an encoder-decoder structure, where each layer in the encoder and decoder contains a multi-head self-attention mechanism.", "expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/2061ea5b43b8478f98abe636d8947484_attention_is_all_you_need_1706.03762.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1706.03762", "predicted_file_url": "/files/library/2061ea5b43b8478f98abe636d8947484_attention_is_all_you_need_1706.03762.pdf", "doc_match": 5, "relevance": 5, "coverage": 5, "related_support": 1, "overall": 4, "feedback": "The `home` document is the correct paper for the Transformer model. The text within the `home` document directly and clearly states that the Transformer uses multi-head attention in the encoder (self-attention layers), decoder (self-attention layers), and between them (encoder-decoder attention layers), which confirms the encoder-decoder structure and the presence of multi-head self-attention in each layer. This provides excellent relevance and complete coverage for the selection.\n\nHowever, all `related` documents provided are exact duplicates of the `home` document. While the information is highly relevant, the related documents do not offer any *additional* or *distinct* helpful context beyond what is already present in the `home` document, leading to a low `related_support` score."}}
{"selection": "We propose ELECTRA, a new pre-training objective that trains a discriminator to distinguish between real input tokens and plausible but fake tokens generated by a small generator network.", "expected_file_contains": "1910.07470", "predicted_file_url": "a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes the ELECTRA pre-training objective, which uses a discriminator and generator network. The home document and all related documents are from the BERT paper (1810.04805) and exclusively describe BERT's Masked Language Model (MLM) pre-training. This is a completely different pre-training approach and model. Therefore, the retrieved documents are incorrect and irrelevant to the selection, providing no coverage or helpful context about ELECTRA."}}
{"selection": "This discriminative approach is more computationally efficient than masked language modeling and achieves stronger results when pre-training with the same amount of compute.", "expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1910.07470", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection states that a 'discriminative approach is more computationally efficient than masked language modeling'. The 'home' document provided is BERT (1810.04805), which *uses* masked language modeling (MLM) as its core pre-training objective. The text provided from the BERT paper describes MLM and its advantages. This is in direct contrast to the selection, which describes a method superior to MLM. Therefore, the 'home' document is incorrect for the given selection. The selection likely originates from a paper like ELECTRA (1910.07470), which introduces a discriminative pre-training task as an alternative to MLM."}}
{"selection": "We introduce DALL·E, a 12-billion parameter version of GPT-3 trained to generate images from text descriptions, using a dataset of text-image pairs.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection describes DALL·E, an image generation model based on GPT-3. The retrieved 'home' document and all 'related' documents are 'bert_1810.04805.pdf', which discusses BERT and OpenAI GPT in the context of language models, not DALL-E or image generation. The content is completely irrelevant to the selection. The expected document (2010.13852) is the DALL·E paper itself, which was not retrieved."}}
{"selection": "DALL·E represents all data, including text and images, as a single stream of tokens, enabling it to model text-to-image generation as a language modeling problem.", "expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "2010.13852", "predicted_file_url": "/files/library/a4c5d5cbdc174b61901140901cc9b756_bert_1810.04805.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection is about DALL·E, which is a text-to-image generation model. The expected file (based on the arXiv ID 2010.13852) is the DALL·E paper. However, the retrieved home document is the BERT paper (1810.04805), which discusses language modeling but not text-to-image generation or DALL·E. The home text is completely irrelevant to the selection. All related documents are identical copies of the irrelevant home document, providing no additional context or coverage."}}
{"selection": "We introduce a neural algorithm of artistic style that can separate and recombine the content and style of arbitrary images.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/9b408d6989d74337bc5df95b0cc8d5e7_1508.02986v1.pdf", "doc_match_auto": true, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/9b408d6989d74337bc5df95b0cc8d5e7_1508.02986v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The selection is from the abstract of 'A Neural Algorithm of Artistic Style' (arXiv:1508.02986). The system correctly identified the `displayName` and `fileUrl` as '1508.02986v1.pdf'. However, the actual text provided for the `home` document (and all `related` documents) is completely different from the content of the actual 'A Neural Algorithm of Artistic Style' paper. The provided text discusses Perceptron algorithms and margins, which is entirely unrelated to neural style transfer. This indicates a severe failure where the system presents content from a different, irrelevant paper, despite correctly identifying the target paper's ID. Therefore, the retrieved content is not relevant, does not cover the idea, and offers no support."}}
{"selection": "The algorithm uses a pre-trained convolutional neural network to define content and style representations, and then synthesizes a new image by jointly optimizing to match both.", "expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match_auto": false, "judge": {"expected_file_contains": "1508.02986", "predicted_file_url": "/files/library/b337806aa8bb4d0ea1951b7d3f8af18c_1910.07470v1.pdf", "doc_match": 1, "relevance": 1, "coverage": 1, "related_support": 1, "overall": 1, "feedback": "The `selection` describes an algorithm for image content and style representation and synthesis, characteristic of neural style transfer (e.g., \"A Neural Algorithm of Artistic Style\", arXiv:1508.02986). However, the `home` document (`1910.07470v1.pdf`) and `related` documents are about AI-based radiomics, deep learning for medical image analysis (brain diseases, glioblastoma), and Gabor filters. These topics are entirely unrelated to the `selection`'s content. Therefore, the `home` document is incorrect, and its text, along with the `related` texts, provides no relevant support or coverage for the `selection`."}}
